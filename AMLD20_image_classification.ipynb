{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<h3>For Google Colab Users</h3>\n",
    "\n",
    "If you are running this notebook on Google Colab, please make sure to do two things:\n",
    "\n",
    "<b>First</b>, switch the runtime to a GPU instance. This can be done by clicking on the menu `Runtime` and selecting `Change runtime type`. In the following popup window, leave the `Runtime type` on Python 3, but under `Hardware accelerator` choose <b>GPU</b>.\n",
    "\n",
    "<b>Second</b>, execute the following code cell to prepare the notebook environment and its dependencies.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    \n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/epfl-exts/amdl20-image-classification.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r 'amdl20-image-classification/downloads' 'amdl20-image-classification/utils.py' .\n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"amdl20-image-classification/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "The **goal** of this hands-on exercise is to provide a general overview of image classifiation and to show you how you can **train a model** and later use it to **classify images** according to your own categories.\n",
    "\n",
    "The whole process is subdivided into 4 sections:\n",
    "\n",
    "1. **Data Preparation**: Collect the data, clean it and prepare it for further analysis.\n",
    "\n",
    "\n",
    "2. **Data Exploration**: Explore the dataset and modify it to your need.\n",
    "\n",
    "\n",
    "3. **Modeling and Analysis**: Model creation, optimization & evaluation. \n",
    "\n",
    "\n",
    "4. **Results Discussion & Exploration**: Investigate model performance and understand results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>: The complete code to this hands-on exercise can be found under <a href=\"https://github.com/epfl-exts/amdl20-image-classification\">github.com/epfl-exts/amdl20-image-classification</a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the programming environment\n",
    "\n",
    "First things first, let's initiate a few libraries that we need during our analyis. The underlying mechanisms of these lines of code are not important here, but in short: We're preparing all the functions we need later on and make sure that the plotting of the figures looks nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    %tensorflow_version 2.x\n",
    "%run utils.py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "\n",
    "The goal of this section is to (1) specify the target categories we want to predict, and to make sure that (2) the data is aggregated, (3) cleaned and (4) stored in a accessible data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Define Class Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Task 4</h3> (after Task 1, 2 & 3 furhter below)\n",
    "\n",
    "The following list specifies the target categories that we want to use for the image classification. For the first execution of this hands-on exercise it is recommended to leave the list as it is, as each new entry will take some time for the data collection.\n",
    "    \n",
    "At a later stagem, feel free to change this list as you like. As a rule of thumb, I would recommend to keep something between 3-8 target classes.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of class labels to use for the image classification\n",
    "class_labels = [\n",
    "               'brown bear',\n",
    "               'polar bear',\n",
    "               'giant panda',\n",
    "               'red panda',\n",
    "               'lion',\n",
    "               'tiger',\n",
    "               'racoon',\n",
    "               'red fox',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Collect Dataset\n",
    "\n",
    "In an ideal case you would collect the data yourself, make sure that it is of high quality and can be used for an image classification project. In the scope of this hands-on exercise we don't have the time for that so we will take a few short cuts.\n",
    "\n",
    "First, we will use the python package [Google Images Download](https://github.com/hardikvasa/google-images-download) to download images of our target classes directly via google's image search service. However, this library only allows the download of the first 100 entries per search term. To bypass this restriction we will augment our intial search term with additional keywords. So, instead of just looking for images of a \"brown bear\", we will look for \"brown bear close up\" and \"brown bear portrait\" images. This additional search terms are specified by the parameter `search_suffix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional search temrs to expand the number of searches\n",
    "search_suffix = 'close up,portrait'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the class labels and the additional search terms are defined, we can go ahead and collect the images from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset and store the images on disk\n",
    "imgs_raw = collect_images(class_labels, suffix=search_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all images are downloaded and the filenames are stored in the `imgs_raw` variable, we can go ahead and take a look at a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the data we've collected\n",
    "plot_images(imgs_raw, n_col=10, n_row=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note</b>: All the images above are squared, i.e. have the same number of pixels in width and height. This is done on purpose, to make all photos \"equal\" and comparable. This restriction means that some of the images that originally were rectangular, were cropped to a squared shape - which explains why some of the images seem to be cutting of the imporant part of the image.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Clean Dataset\n",
    "\n",
    "Almost all datasets are to a certain degree \"dirty\" and need to be cleaned. Cleaning in the context of classification means to:\n",
    "- remove duplicates\n",
    "- remove outliers or extreme values that are not part of the target population\n",
    "- hanlde of missing values\n",
    "\n",
    "We don't have any missing values in these dataset (images are either downloaded or not), and will therefore only focus on the first two points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from the dataset\n",
    "imgs_unique = remove_duplicates(imgs_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers\n",
    "\n",
    "The removal of outliers is a bit more tricky when it comes to images. Ideally you would do this by hand, to make sure that you only keep meaningful images in your dataset, i.e. images that belong to one of your target categories. For this hands-on exercise, a manual check-up is not possible, so let's try a quick shortcut by looking at the color profile of the images.\n",
    "\n",
    "Each image has a particular color profile, i.e. distribution of red, green and blue (RGB) pixel values. Looking at these particular RGB value distributions might allow us to differentiate photos of animals from heavily photoshopped images, gray colored images or graphical logos. Let's take a closer look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot images together with their RGB color distributions\n",
    "plot_images(imgs_unique, n_col=8, n_row=2, show_histogram=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove outlier images from the dataset, we will take a look at the RGB color profile of each image and remove images that either are only in gray colors or that have sudden extreme spikes in the RGB profile (i.e. images that have huge amounts of pixels with the exact same color value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from the dataset\n",
    "imgs_clean, imgs_outlier = remove_outliers(imgs_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we separated the dataset into clean and outlier images, let's take a closer look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot some outlier images\n",
    "plot_images(imgs_outlier, n_col=10, n_row=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some clean images\n",
    "plot_images(imgs_clean, n_col=10, n_row=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Finalize the Dataset\n",
    "\n",
    "As a last step in the **data preparation** section, we need to finalize the actual dataset and store the relevant image information in useful variables, conventionally called `X` and `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Task 3</h3> (after Task 1 & 2 furhter below)\n",
    "\n",
    "The image dimension of 32 in the next cell was chosen as a trade-off between speed and image detail. To better understand the effect of this trade-off we recommend you to explore different values for <code>img_dim=32</code>. What happens if you set this number to 16, 64 or even 128?\n",
    "\n",
    "<b>Note</b>: Changing this parameter will also have an effect on the computation time. For this reason we recommend that during the first walk through of this hands-on that you leave this parameter at <code>img_dim=32</code>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X_pixel, y_pixel, metainfo = create_dataset(imgs_clean, class_labels, img_dim=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note 1:</b> In machine learning, the variable <code>X</code> is usually used as the dataframe, i.e. the feature matrix that contains the detailed information about our samples. The variable <code>y</code> is used as the target variable, in this case the label for the class.\n",
    "\n",
    "Here, we've added an additional variable called <code>metainfo</code> that contains relevant information about the dataset (e.g. size of dataset, class labels, image filenames, etc.) which is unique to this exercise, and will be needed in the code further below.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Note 2:</b> The function <code>create_dataset</code> above also contains the parameter <code>img_dim=32</code>, which specified to which pixel resolution the images should be resampled. In this case, <code>32</code> means that all images are resized to a pixel resolution of 32 x 32. Downsampling images from their original size to a size of 32 x 32 reduces the computation time, and makes sure that all images have the same size.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Exploration\n",
    "\n",
    "Once the dataset is prepared and cleaned, it is important that we make sure that we understand its properties. This can be done by doing something called an **Explorative Data Analysis** (EDA). Each of our investigations during the EDA should lead to an observation, followed by a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Data Description\n",
    "\n",
    "So, first things first, let's take a closer look at our dataset and describe it's basic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many images per class do we have?\n",
    "plot_class_distribution(y_pixel, metainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The classes are not equally distributed. Some classes have more than twice as many images than others.  \n",
    "**Decision**: The unbalanced occurence of the different classes needs to be considered during the data modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the average image per target category look like?\n",
    "plot_class_average(X_pixel, y_pixel, metainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Some classes like `giant_panda`, `polar_bear` and `red_panda` have very unique color characteristics.  \n",
    "**Decision**: Let's take a closer look at the RGB color distribution of these class average images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the average RGB color profile look like per class?\n",
    "plot_class_RGB(X_pixel, y_pixel, metainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The individual RGB color distributions of almost all classes look unique.  \n",
    "**Decision**: We should use the RGB color profile as a data feature for the image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Features\n",
    "\n",
    "From the previous section, we know that the RGB color profile of each image might be useful to identify the image's class. For this reason, let's take the first dataset `X_pixel` and create a second dataset called `X_rgb` that consists only of these RGB color profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RGB color profiles for each image individually\n",
    "X_rgb, y_rgb = extract_RGB_features(y_pixel, metainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature from a Convolutional Neural Network\n",
    "\n",
    "In addition to the RGB color profile as a new data feature, let's also try to extract meaningful features from our images by using a state of the art neural network called [MobileNetV2](https://arxiv.org/abs/1801.04381). **Note**, the neural network that we will use for this feature extraction will be downloaded directly from [Tensorflow Hub](https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract features according to MobileNetV2 (Convolutional Neural Network)\n",
    "X_nn, y_nn = extract_neural_network_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Recap before Data Modeling\n",
    "\n",
    "Before we continue with the next section, let's quickly recap the three different ways in which we've represented our images:\n",
    "- `X_pixel`: This dataset contains the original data in its raw form. Each value is a RGB pixel value in the original image.\n",
    "- `X_rgb`: This dataset contains the RGB color profile of each image.\n",
    "- `X_nn`: This dataset contains an 1280-dimensional feature vectore per image, extracted with a convolutional neural network called MobileNetV2.\n",
    "\n",
    "To better understand what this actually means, let's randomly select an image and plot these three representations next to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_recap(X_pixel, X_rgb, X_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling and Analysis\n",
    "\n",
    "Once the data is (1) collected, cleaned, and prepared and (2) meaningfull features are extracted - we are ready for the data modeling. In the following section we will do the model fitting and optimization once for each of these three dataset, i.e. `X_pixel`, `X_rgb` and `X_nn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>More indepth information about the classifier:</b> The model used in this hands-on example is a [Ridge Regression Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html). Ridge regression has the advantage of being rather quick, while also allowing us the fine tuning of a regularization parameter alpha for optimal data fitting. During model optimization, we will be using a crossvalidation approach with 4 folds to fine tune the hyperparameter. To allow to test for generalization, the dataset is initially split into a training and test set according to a 50/50 ratio. Because of the unequal distribution of the class labels throughout the dataset a weighted balancing approach during model optimization is applied.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Model Fitting on `X_pixel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train a model fit to the data in pixel format\n",
    "model_pixel = model_fit(X_pixel, y_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigation of the model performance\n",
    "check_model_performance(model_pixel, metainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Model Fitting on `X_rgb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model fit to the data in rgb format\n",
    "model_rgb = model_fit(X_rgb, y_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigation of the model performance\n",
    "check_model_performance(model_rgb, metainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Model Fitting on `X_nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model fit to the data in neural network format\n",
    "model_nn = model_fit(X_nn, y_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigation of the model performance\n",
    "check_model_performance(model_nn, metainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results Discussion & Exploration\n",
    "\n",
    "## 4.1. Investigation of Model Performance\n",
    "\n",
    "Once the model was optimized and the general model performance was investigated, we should take a closer look at the results: Why did the model predict a given class? With what certainty was this decision made? Which images did the model missclassify and why? Which images are easy or difficult for our model? etc.\n",
    "\n",
    "So, let's investigate the results and why a model chose one class over the others. For this, let's select one of the three models to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Task 1 </h3>\n",
    "\n",
    "The `model_nn` should lead to the best results, so we should investigate its results first. Nonetheless, try to investigate the model performance for the `model_pixel` and `model_rgb` as well by changing the variable name in the following cell and observing the output changes in the following two cells.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which model to investigate: 'model_pixel', 'model_rgb' or 'model_nn'\n",
    "model = model_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which images the model did correctly classify and how certain it was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correct predictions\n",
    "investigate_predictions(model, metainfo, show_correct=True, nimg=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which images the model did not classify correctly and why it was wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wrong predictions\n",
    "investigate_predictions(model, metainfo, show_correct=False, nimg=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Apply Model to new Images from the web\n",
    "\n",
    "The initial training of the model can be very computational intensive. But once the model is trained, using it to predict the class of new images goes very quickly.\n",
    "\n",
    "So, let's take an image from the web and see how well our trained model performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class properties of an image from the web\n",
    "img_url = 'https://c402277.ssl.cf1.rackcdn.com/photos/18134/images/hero_small/Medium_WW226365.jpg?1574452099'\n",
    "predict_new_image(img_url, model_nn, metainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about an image that is only slightly related to the target categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class properties of an image from the web\n",
    "img_url = 'https://live.staticflickr.com/7279/7017467025_8807cc82f6_b.jpg'\n",
    "predict_new_image(img_url, model_nn, metainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you always wondered what kind of animal you are, just plug in your image below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class properties of another image from the web\n",
    "img_url = 'https://d33wubrfki0l68.cloudfront.net/067d6b185769f031404e927b1b70de6d5ece3e0a/d82f4/michael.1164620e.jpg'\n",
    "predict_new_image(img_url, model_nn, metainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Task 2</h3>\n",
    "    \n",
    "Choose your own image from the internet by storing its URL under the variable `img_url` and run the following cell.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = 'www.YOUR_OWN_IMAGE.jpg'\n",
    "predict_new_image(img_url, model_nn, metainfo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
